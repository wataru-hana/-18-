# 価格自動取得システムのウェブアプリ化・デプロイ計画

## 📋 概要

現在のコマンドライン実行型のスクリプトを、ウェブアプリケーションとしてデプロイ可能にするための計画書です。

## ✅ デプロイ可能か？

**はい、デプロイ可能です。** ただし、いくつかの変更が必要です。

## 🏗️ アーキテクチャの変更

### 現在のアーキテクチャ
```
コマンドライン実行
  ↓
ローカルファイルシステム
  - Excelファイル
  - YAML設定ファイル
  - ログファイル
```

### ウェブアプリ化後のアーキテクチャ
```
Webブラウザ
  ↓
Webサーバー (Flask/FastAPI)
  ↓
バックグラウンドジョブ (Celery)
  ↓
データベース (SQLite/PostgreSQL)
  + ファイルストレージ (S3/ローカル)
```

## 🔧 必要な変更点

### 1. Webフレームワークの追加

**推奨**: Flask または FastAPI

```python
# Flask の例
from flask import Flask, render_template, jsonify, request
from flask_celery import make_celery

app = Flask(__name__)
app.config['CELERY_BROKER_URL'] = 'redis://localhost:6379/0'
celery = make_celery(app)
```

### 2. データベースの導入

**現在**: Excelファイルに直接書き込み
**変更後**: データベースに保存

**推奨データベース**:
- **SQLite**: 小規模・開発環境向け（簡単）
- **PostgreSQL**: 本番環境向け（スケーラブル）

**データモデル例**:
```python
# SQLAlchemy モデル例
class Company(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    name = db.Column(db.String(100), nullable=False)
    region = db.Column(db.String(50))
    price_url = db.Column(db.String(500))
    category = db.Column(db.Integer)
    extractor_type = db.Column(db.String(50))

class PriceData(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    company_id = db.Column(db.Integer, db.ForeignKey('company.id'))
    material_name = db.Column(db.String(100))
    price = db.Column(db.String(50))
    scraped_at = db.Column(db.DateTime)
```

### 3. バックグラウンドジョブの実装

**現在**: 同期的にスクレイピング実行
**変更後**: 非同期でバックグラウンド実行

**推奨**: Celery + Redis/RabbitMQ

```python
@celery.task
def scrape_prices_task(company_ids=None):
    """バックグラウンドでスクレイピングを実行"""
    # スクレイピング処理
    results = scrape_companies(company_ids)
    # データベースに保存
    save_to_database(results)
    return results
```

### 4. フロントエンドの作成

**必要な機能**:
- スクレイピング実行ボタン
- 進捗表示
- 結果表示（テーブル形式）
- Excelダウンロード機能
- 設定管理画面

**推奨技術**:
- **シンプル**: HTML + JavaScript (Vanilla JS)
- **モダン**: React/Vue.js
- **簡単**: Bootstrap + jQuery

### 5. ファイルストレージの変更

**現在**: ローカルファイルシステム
**変更後**: 
- **開発環境**: ローカルファイルシステム
- **本番環境**: AWS S3 / Google Cloud Storage

### 6. 設定ファイルの管理

**現在**: YAMLファイルを直接読み込み
**変更後**: 
- データベースに保存
- または環境変数で管理
- 管理画面から編集可能に

## 📦 実装例（Flask版）

### 基本的な構造

```
webapp/
├── app.py                 # Flaskアプリケーション
├── models.py              # データベースモデル
├── tasks.py               # Celeryタスク
├── routes.py              # ルーティング
├── templates/             # HTMLテンプレート
│   ├── index.html
│   ├── results.html
│   └── settings.html
├── static/                # CSS/JavaScript
│   ├── css/
│   └── js/
├── scrapers/              # 既存のスクレイパー（そのまま使用可能）
└── config/                # 設定ファイル（そのまま使用可能）
```

### app.py の例

```python
from flask import Flask, render_template, jsonify, request, send_file
from flask_sqlalchemy import SQLAlchemy
from celery import Celery
import os

app = Flask(__name__)
app.config['SQLALCHEMY_DATABASE_URI'] = os.getenv('DATABASE_URL', 'sqlite:///prices.db')
app.config['CELERY_BROKER_URL'] = os.getenv('CELERY_BROKER_URL', 'redis://localhost:6379/0')

db = SQLAlchemy(app)
celery = Celery(app.name, broker=app.config['CELERY_BROKER_URL'])

from models import Company, PriceData
from tasks import scrape_prices_task

@app.route('/')
def index():
    """メインページ"""
    companies = Company.query.all()
    return render_template('index.html', companies=companies)

@app.route('/api/scrape', methods=['POST'])
def start_scraping():
    """スクレイピングを開始"""
    company_ids = request.json.get('company_ids', None)
    task = scrape_prices_task.delay(company_ids)
    return jsonify({'task_id': task.id}), 202

@app.route('/api/status/<task_id>')
def task_status(task_id):
    """タスクの状態を取得"""
    task = scrape_prices_task.AsyncResult(task_id)
    return jsonify({
        'status': task.status,
        'result': task.result if task.ready() else None
    })

@app.route('/api/results')
def get_results():
    """結果を取得"""
    results = PriceData.query.order_by(PriceData.scraped_at.desc()).all()
    return jsonify([{
        'company': r.company.name,
        'material': r.material_name,
        'price': r.price,
        'scraped_at': r.scraped_at.isoformat()
    } for r in results])

@app.route('/api/download/excel')
def download_excel():
    """Excelファイルをダウンロード"""
    # Excelファイルを生成
    excel_file = generate_excel_file()
    return send_file(excel_file, as_attachment=True, 
                     download_name='price_results.xlsx')
```

### tasks.py の例

```python
from app import celery
from scrapers import Category1Scraper, Category2Scraper
import yaml

@celery.task(bind=True)
def scrape_prices_task(self, company_ids=None):
    """バックグラウンドでスクレイピングを実行"""
    # 設定ファイルを読み込み（既存のコードを再利用）
    with open('config/sites.yaml', 'r', encoding='utf-8') as f:
        sites_config = yaml.safe_load(f)
        sites = sites_config.get('sites', [])
    
    # 指定された企業のみフィルタリング
    if company_ids:
        sites = [s for s in sites if s.get('id') in company_ids]
    
    results = []
    total = len(sites)
    
    for i, site_config in enumerate(sites):
        # 進捗を更新
        self.update_state(state='PROGRESS', 
                         meta={'current': i+1, 'total': total})
        
        # スクレイピング実行（既存のコードを再利用）
        category = site_config.get('category', 2)
        if category == 1:
            scraper = Category1Scraper(site_config, delay=2.0)
        else:
            scraper = Category2Scraper(site_config, delay=2.0)
        
        result = scraper.scrape(filter_target_items=True)
        results.append(result)
    
    # データベースに保存
    save_to_database(results)
    
    return {'status': 'completed', 'results': results}
```

## 🚀 デプロイ方法

### 1. ローカル開発環境

```bash
# 依存関係のインストール
pip install flask flask-sqlalchemy celery redis

# Redisの起動（別ターミナル）
redis-server

# Celeryワーカーの起動（別ターミナル）
celery -A app.celery worker --loglevel=info

# Flaskアプリの起動
python app.py
```

### 2. Docker化（推奨）

```dockerfile
# Dockerfile
FROM python:3.9-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["gunicorn", "-w", "4", "-b", "0.0.0.0:5000", "app:app"]
```

```yaml
# docker-compose.yml
version: '3.8'
services:
  web:
    build: .
    ports:
      - "5000:5000"
    environment:
      - DATABASE_URL=postgresql://user:pass@db:5432/prices
      - CELERY_BROKER_URL=redis://redis:6379/0
    depends_on:
      - db
      - redis
  
  worker:
    build: .
    command: celery -A app.celery worker --loglevel=info
    environment:
      - DATABASE_URL=postgresql://user:pass@db:5432/prices
      - CELERY_BROKER_URL=redis://redis:6379/0
    depends_on:
      - db
      - redis
  
  db:
    image: postgres:13
    environment:
      - POSTGRES_DB=prices
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=pass
    volumes:
      - postgres_data:/var/lib/postgresql/data
  
  redis:
    image: redis:7-alpine
  
volumes:
  postgres_data:
```

### 3. クラウドデプロイ

#### Heroku（簡単）

```bash
# Procfile
web: gunicorn app:app
worker: celery -A app.celery worker --loglevel=info

# デプロイ
heroku create price-scraper-app
heroku addons:create heroku-postgresql
heroku addons:create heroku-redis
git push heroku main
```

#### AWS（本格的）

- **EC2**: WebサーバーとCeleryワーカー
- **RDS**: PostgreSQLデータベース
- **ElastiCache**: Redis
- **S3**: ファイルストレージ
- **ECS/EKS**: コンテナオーケストレーション

#### Google Cloud Platform

- **Cloud Run**: サーバーレスコンテナ
- **Cloud SQL**: PostgreSQL
- **Cloud Memorystore**: Redis
- **Cloud Storage**: ファイルストレージ

## 📝 必要な追加パッケージ

```txt
# requirements_web.txt
flask>=2.3.0
flask-sqlalchemy>=3.0.0
flask-cors>=4.0.0
celery>=5.3.0
redis>=5.0.0
gunicorn>=21.2.0
psycopg2-binary>=2.9.0  # PostgreSQL用
```

## ⚠️ 注意事項

### 1. スクレイピングの法的・倫理的考慮

- 各サイトの利用規約を確認
- robots.txtを遵守
- 適切な待機時間を設定（現在2秒）
- 過度な負荷をかけない

### 2. セキュリティ

- 認証・認可の実装（ログイン機能）
- APIキーの保護
- SQLインジェクション対策（SQLAlchemyで自動対応）
- XSS対策（テンプレートエンジンで自動対応）

### 3. パフォーマンス

- スクレイピングは時間がかかる（18社で約36秒）
- バックグラウンドジョブで非同期処理
- 結果をキャッシュ（一定時間は再実行しない）

### 4. エラーハンドリング

- ネットワークエラー
- タイムアウト
- HTML構造の変更
- サイトのダウン

## 🎯 実装の優先順位

### Phase 1: 最小限のウェブアプリ（MVP）
1. Flaskアプリの基本構造
2. スクレイピング実行API
3. 結果表示ページ
4. SQLiteデータベース

### Phase 2: 機能追加
1. バックグラウンドジョブ（Celery）
2. 進捗表示
3. Excelダウンロード機能
4. 設定管理画面

### Phase 3: 本番環境対応
1. PostgreSQLデータベース
2. 認証・認可
3. ログ機能
4. モニタリング

## 💡 推奨アプローチ

1. **まずはローカルで動作確認**
   - Flask + SQLite + 同期的なスクレイピング
   - 既存のスクレイパーコードをそのまま使用

2. **バックグラウンドジョブを追加**
   - Celery + Redis
   - 進捗表示機能

3. **本番環境にデプロイ**
   - Docker化
   - クラウドサービスにデプロイ

## 📚 参考リソース

- Flask公式ドキュメント: https://flask.palletsprojects.com/
- Celery公式ドキュメント: https://docs.celeryq.dev/
- FastAPI公式ドキュメント: https://fastapi.tiangolo.com/

## まとめ

**デプロイ可能**: ✅ はい
**難易度**: 中程度（既存コードの大部分は再利用可能）
**推奨**: Flask + Celery + PostgreSQL + Docker

既存のスクレイパーコード（`scrapers/`ディレクトリ）はそのまま使用できるため、Webフレームワークの部分だけを追加すれば実現可能です。








